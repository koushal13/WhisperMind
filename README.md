# ğŸ§  WhisperMind - Local Knowledge Chatbot with Voice Input

A privacy-first, offline AI chatbot that combines the power of Ollama's local language models with Retrieval Augmented Generation (RAG) and voice capabilities. WhisperMind can answer questions from your personal documents and interact with you through voice, all while keeping your data completely private and offline.

## âœ¨ Features

- **ğŸ”’ Privacy-First**: Runs entirely offline - no cloud dependencies
- **ğŸ“š RAG-Powered**: Answer questions from your local documents
- **ğŸ¤ Voice Input**: Speech-to-text using OpenAI Whisper
- **ğŸ”Š Voice Output**: Text-to-speech using Coqui TTS
- **ğŸŒ Web Interface**: Beautiful Streamlit-based UI
- **ğŸ’» CLI Support**: Command-line interface for power users
- **ğŸ“„ Multi-Format**: Supports PDF, DOCX, TXT, MD, HTML files
- **ğŸš€ Local Models**: Powered by Ollama (Llama3, Mistral, etc.)

## ğŸ› ï¸ Tech Stack

- **Language**: Python 3.8+
- **LLM**: Ollama (Llama3, Mistral, CodeLlama)
- **Vector Database**: ChromaDB for document embeddings
- **Speech-to-Text**: OpenAI Whisper
- **Text-to-Speech**: Coqui TTS
- **Web UI**: Streamlit
- **Document Processing**: PyPDF, python-docx, BeautifulSoup4

## ğŸ“ Project Structure

```
WhisperMind/
â”œâ”€â”€ src/                    # Main application code
â”‚   â”œâ”€â”€ core/              # Core components
â”‚   â”‚   â”œâ”€â”€ ollama_client.py    # Ollama API client
â”‚   â”‚   â””â”€â”€ config.py           # Configuration management
â”‚   â”œâ”€â”€ rag/               # RAG system
â”‚   â”‚   â”œâ”€â”€ document_processor.py  # Document processing
â”‚   â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB vector store
â”‚   â”‚   â””â”€â”€ retriever.py         # Document retrieval
â”‚   â”œâ”€â”€ voice/             # Voice processing
â”‚   â”‚   â”œâ”€â”€ speech_to_text.py    # Whisper STT
â”‚   â”‚   â””â”€â”€ text_to_speech.py    # Coqui TTS
â”‚   â”œâ”€â”€ ui/                # User interfaces
â”‚   â”‚   â””â”€â”€ streamlit_app.py     # Streamlit web UI
â”‚   â””â”€â”€ chatbot.py         # Main chatbot class
â”œâ”€â”€ data/                  # Your documents
â”‚   â””â”€â”€ documents/         # Place your files here
â”œâ”€â”€ models/                # Downloaded models
â”œâ”€â”€ config/                # Configuration files
â”‚   â””â”€â”€ config.yaml        # Main configuration
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ main.py               # CLI entry point
â””â”€â”€ README.md             # This file
```

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.8+**
2. **Ollama** - Download from [ollama.com](https://ollama.com)
3. **Git** (optional)

### Installation

1. **Clone or download the project**
   ```bash
   git clone <repository-url>
   cd Silent-Canoe-WhisperMind
   ```

2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install and start Ollama**
   ```bash
   # Download Ollama from https://ollama.com
   # Then pull a model (this may take a few minutes)
   ollama pull llama3
   ```

4. **Copy environment configuration**
   ```bash
   cp .env.example .env
   # Edit .env if needed
   ```

5. **Add your documents**
   ```bash
   # Place your documents in data/documents/
   # Supported formats: PDF, DOCX, TXT, MD, HTML
   ```

### Usage

#### Web Interface (Recommended)

Launch the beautiful Streamlit web interface:

```bash
python main.py --ui
```

Then open your browser to `http://localhost:8501`

#### Command Line Interface

For a simple CLI chat experience:

```bash
python main.py
```

#### Load Documents Only

To process documents without starting chat:

```bash
python main.py --load-docs data/documents
```

#### Test the System

Run a quick test to verify everything works:

```bash
python main.py --test
```

## ğŸ“š Adding Documents

1. Place your documents in the `data/documents/` folder
2. Supported formats:
   - **PDF** files (`.pdf`)
   - **Word documents** (`.docx`)
   - **Text files** (`.txt`)
   - **Markdown** (`.md`)
   - **HTML** (`.html`, `.htm`)

3. The system will automatically process and index them for search

## âš™ï¸ Configuration

Edit `config/config.yaml` to customize:

```yaml
# Example configuration
ollama:
  model: "llama3"  # or mistral, codellama, etc.
  temperature: 0.7
  
voice:
  enabled: true
  whisper_model: "base"  # tiny, base, small, medium, large
  
rag:
  top_k: 5  # Number of documents to retrieve
  similarity_threshold: 0.7
```

## ğŸ¤ Voice Features

### Speech-to-Text (Whisper)
- Models: `tiny`, `base`, `small`, `medium`, `large`
- Automatic language detection
- High accuracy transcription

### Text-to-Speech (Coqui TTS)
- Natural-sounding voices
- Multiple language support
- Configurable speakers

## ğŸ”§ Advanced Usage

### Custom Ollama Models

```bash
# Use different models
ollama pull mistral
ollama pull codellama
```

Then update `config/config.yaml`:
```yaml
ollama:
  model: "mistral"
```

### GPU Acceleration

For faster processing with NVIDIA GPUs:

```bash
# Install CUDA-enabled PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Custom Embedding Models

Edit the configuration to use different embedding models:

```yaml
chromadb:
  embedding_model: "all-mpnet-base-v2"  # Higher quality embeddings
```

## ğŸ› Troubleshooting

### Common Issues

1. **Ollama not found**
   - Make sure Ollama is installed and running
   - Check if `ollama` command works in terminal

2. **Model not available**
   - Pull the model: `ollama pull llama3`
   - Check available models: `ollama list`

3. **Voice features not working**
   - Install additional audio dependencies:
     ```bash
     pip install librosa soundfile
     ```

4. **Out of memory errors**
   - Use smaller models (`tiny` Whisper, smaller Ollama models)
   - Reduce `chunk_size` in configuration

5. **Slow performance**
   - Use GPU if available
   - Reduce `max_tokens` in configuration
   - Use smaller models

### Performance Tips

- **For laptops**: Use `whisper: tiny` and `ollama: llama3:8b`
- **For desktops**: Use `whisper: base` and `ollama: llama3:70b`
- **For servers**: Use `whisper: large` and largest available models

## ğŸ§ª Development

### Running Tests

```bash
pip install pytest pytest-asyncio
pytest tests/
```

### Code Formatting

```bash
pip install black flake8
black src/
flake8 src/
```

### Adding New Document Types

Extend `src/rag/document_processor.py` to support additional formats.

## ğŸ“‹ Requirements

- **System**: Windows 10+, macOS 10.14+, or Linux
- **RAM**: 8GB minimum, 16GB recommended
- **Storage**: 10GB+ for models and documents
- **Python**: 3.8 or higher

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is open source. Feel free to use, modify, and distribute.

## ğŸ™ Acknowledgments

- **Ollama** - Local LLM inference
- **OpenAI Whisper** - Speech recognition
- **Coqui TTS** - Text-to-speech synthesis
- **ChromaDB** - Vector database
- **Streamlit** - Web interface framework

## ğŸ“ Support

- Check the [Issues](issues) page for known problems
- Create a new issue for bugs or feature requests
- Join discussions in the community

---

**Happy chatting with WhisperMind! ğŸ§ ğŸ’¬**